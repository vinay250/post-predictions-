{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\endtoendpost'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"]=\"https://dagshub.com/vinayakavirat008/post-predictions-.mlflow \"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"]=\"vinayakavirat008\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"]=\"8006a8c9e77082e90d96573a13bb278110094dff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "    mlflow_uri: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.postpredictions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        params = self.params.ElasticNet\n",
    "        schema =  self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path = config.model_path,\n",
    "            all_params=params,\n",
    "            metric_file_name = config.metric_file_name,\n",
    "            target_column = schema.name,\n",
    "            mlflow_uri=\"https://dagshub.com/vinayakavirat008/post-predictions-.mlflow\"\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from source.postpredictions.utils.utils import load_object\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def log_info(self, message):\n",
    "        mlflow.log_text(\"info\", message)\n",
    "\n",
    "    def initiate_model_evaluation(self, train_array, test_array):\n",
    "        try:\n",
    "            # Assuming 'Emotion' is the target column\n",
    "            target_column = 'Emotion'\n",
    "\n",
    "            # Check if the target column is present in the DataFrame\n",
    "            if target_column not in test_array.columns:\n",
    "                raise ValueError(f\"The target column '{target_column}' is not present in the DataFrame.\")\n",
    "\n",
    "            # Split features (X_test) and target variable (y_test) from the test_array\n",
    "            X_test = test_array.drop(columns=[target_column])\n",
    "            y_test = test_array[target_column]\n",
    "\n",
    "            # Load the trained model\n",
    "            model_path = os.path.join(\"artifacts\", \"model.pkl\")\n",
    "            model = load_object(model_path)\n",
    "\n",
    "            # Set the MLflow registry URI\n",
    "            mlflow.set_registry_uri(\"https://dagshub.com/vinayakavirat008/post-predictions-.mlflow\")\n",
    "            \n",
    "            # Get the type of the tracking URI\n",
    "            tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "            \n",
    "            self.log_info(f\"Tracking URL Type: {tracking_url_type_store}\")\n",
    "\n",
    "            # Start a new MLflow run\n",
    "            with mlflow.start_run():\n",
    "                print(mlflow.get_tracking_uri())\n",
    "\n",
    "                self.log_info(\"Making predictions using the test set\")\n",
    "                predicted_qualities = model.predict(X_test)\n",
    "\n",
    "                self.log_info(\"Evaluating the model and logging classification metrics\")\n",
    "                accuracy = accuracy_score(y_test, predicted_qualities)\n",
    "                classification_rep = classification_report(y_test, predicted_qualities)\n",
    "                confusion_mat = confusion_matrix(y_test, predicted_qualities)\n",
    "\n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "                # If using a remote registry, register the model in the Model Registry\n",
    "                if tracking_url_type_store != \"file\":\n",
    "                    mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"ml_model\")\n",
    "                else:\n",
    "                    # If using a local file store, log the model without registering\n",
    "                    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "                # Log classification metrics\n",
    "                mlflow.log_text(\"classification_report\", classification_rep)\n",
    "                mlflow.log_artifact(confusion_mat, \"confusion_matrix.txt\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.log_info(f\"Exception occurred: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
